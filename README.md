## Supply Chain Forecasting and Optimization

This repository contains an end-to-end workflow for **demand forecasting and optimization in a supply chain context**, implemented as a sequence of Jupyter notebooks. It covers data loading and validation, exploratory data analysis, feature engineering, and modeling with LSTM, XGBoost, and a hybrid ensemble.

### Repository structure

- **`01_data_loading_and_validation.ipynb`**: Loads the raw supply chain dataset, performs basic cleaning, handles missing values, validates data types and ranges, and saves a cleaned/validated version of the data for downstream steps.
- **`02_eda.ipynb`**: Exploratory data analysis of the cleaned dataset, including univariate and multivariate summaries, time series behavior, seasonal patterns, and relationships between key operational variables.
- **`03_feature_engineering.ipynb`**: Creates modeling features such as time-based lags, rolling statistics, categorical encodings, and domain-specific transformations used by the forecasting models.
- **`04_lstm_model.ipynb`**: Builds and trains an LSTM-based deep learning model for time-series forecasting, including train/validation splits, model architecture, training loops, and evaluation metrics. Saves the best model, test predictions, and per-SKU performance.
- **`05_xgboost_model.ipynb`**: Trains an XGBoost model using the engineered features as a strong baseline. Saves the tuned model, best hyperparameters, test predictions, and feature importance.
- **`06_hybrid_model.ipynb`**: Combines LSTM and XGBoost predictions (e.g., via a meta-model or weighted average) to produce a hybrid forecast. Saves hybrid predictions, metrics, and model comparison (e.g., `all_model_comparison.csv`).
- **`data/` (ignored in git)**: Expected location for raw data files such as `supply_chain_dataset1.csv`. This folder is excluded from version control; place your data here locally.

### Data

- **Input data**: The notebooks expect a supply chain dataset (for example, `data/supply_chain_dataset1.csv`) containing the historical observations used for forecasting.
- **Validated data**: A cleaned/validated dataset (for example, `supply_chain_validated.csv`) may be generated by `01_data_loading_and_validation.ipynb` and reused by later notebooks.
- **Git ignore**: The `data/` directory is ignored by `.gitignore`, so datasets are not committed to the repository by default. You will need to provide the data locally before running the notebooks.
- **Dataset (Currently using Synthetic [Dataset](https://www.kaggle.com/datasets/ziya07/high-dimensional-supply-chain-inventory-dataset))**

### Environment setup

1. **Install Python**
   - Use **Python 3.9+**.

2. **Create and activate a virtual environment (recommended)**

   ```bash
   python -m venv .venv
   # Windows (PowerShell)
   .venv\Scripts\Activate.ps1
   # Or, Windows (cmd)
   .venv\Scripts\activate.bat
   ```

3. **Install dependencies**

   If you have a `requirements.txt` file, run:

   ```bash
   pip install -r requirements.txt
   ```

   Otherwise, install commonly used libraries for this workflow:

   ```bash
   pip install pandas numpy scikit-learn matplotlib seaborn xgboost tensorflow jupyter
   ```

### Running the notebooks

1. Place your input dataset(s) in the `data/` directory (for example, `data/supply_chain_dataset1.csv`).
2. Start Jupyter:

   ```bash
   jupyter lab
   # or
   jupyter notebook
   ```

3. Open and run the notebooks in order:
   1. `01_data_loading_and_validation.ipynb`
   2. `02_eda.ipynb`
   3. `03_feature_engineering.ipynb`
   4. `04_lstm_model.ipynb`
   5. `05_xgboost_model.ipynb`
   6. `06_hybrid_model.ipynb`

Running them sequentially ensures that each later notebook can reuse the outputs (cleaned data, engineered features, and saved models) from earlier steps.

### Modeling overview

- **LSTM** (`04_lstm_model.ipynb`): Sequence-based deep learning for time-series demand forecasting; captures temporal dependencies, trends, and seasonality. Outputs: `lstm_best_model.keras` (or `lstm_final_model.keras`), `lstm_test_predictions.csv`, `lstm_metrics.json`, `lstm_sku_performance.csv`.

- **XGBoost** (`05_xgboost_model.ipynb`): Gradient-boosted trees on engineered features; strong baseline and interpretable feature importance. Outputs: `xgb_final_model.pkl`, `xgb_best_params.json`, `xgb_test_predictions.csv`, `xgb_metrics.json`, `xgb_sku_performance.csv`.

- **Hybrid** (`06_hybrid_model.ipynb`): Combines LSTM and XGBoost predictions (e.g., meta-model or weighted average) for a single ensemble forecast. Outputs: `hybrid_predictions.csv`, `hybrid_metrics.json`, `all_model_comparison.csv`, and optional meta-model artifacts.

Compare models using MAE, MAPE, RMSE, or other metrics in the notebooks and in `all_model_comparison.csv` to choose the best approach for your use case.

### Reproducibility and extensions

- **Random seeds**: If you want strict reproducibility, ensure seeds are set for NumPy, TensorFlow, and XGBoost where appropriate.
- **Hyperparameter tuning**: You can extend the modeling notebooks with grid search, random search, or Bayesian optimization for better performance.
- **Operational integration**: The modeling outputs can be integrated into downstream optimization or planning tools (e.g., safety stock calculation, replenishment policies, scenario analysis).


This repository is updated frequently; run notebooks in order and ensure outputs from earlier steps exist before running later ones.

### License

Specify your preferred license (e.g., MIT, Apache-2.0, or proprietary) and add a `LICENSE` file if needed.

